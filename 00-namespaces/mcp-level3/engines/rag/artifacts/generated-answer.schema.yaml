# Generated Answer Artifact Schema
# Defines the structure for RAG-generated answers with provenance and evaluation metrics

version: "1.0.0"
semantic_role: "generated_answer"
artifact_type: "schema"
semantic_root: true

metadata:
  name: "generated-answer"
  description: "Schema for answers generated by RAG systems with full provenance and quality metrics"
  author: "MCP Core Team"
  created: "2025-01-11"
  updated: "2025-01-11"
  tags:
    - "rag"
    - "generation"
    - "answer"
    - "llm"

schema:
  type: "object"
  required:
    - "answer_id"
    - "query"
    - "answer_text"
    - "context"
    - "generation_metadata"
  properties:
    answer_id:
      type: "string"
      description: "Unique identifier for the generated answer"
      pattern: "^answer:[a-zA-Z0-9_-]+:[0-9]+$"
      example: "answer:query123:001"
      
    query:
      type: "object"
      description: "Original query information"
      required:
        - "query_id"
        - "query_text"
      properties:
        query_id:
          type: "string"
          description: "Unique query identifier"
          example: "query:123"
          
        query_text:
          type: "string"
          description: "Original query text"
          example: "What are the applications of machine learning in healthcare?"
          
        query_timestamp:
          type: "string"
          format: "date-time"
          description: "When the query was received"
          example: "2025-01-11T10:30:00Z"
          
        user_id:
          type: "string"
          description: "User who submitted the query"
          example: "user:john_doe"
          
        session_id:
          type: "string"
          description: "Session identifier for conversation tracking"
          example: "session:abc123"
          
    answer_text:
      type: "string"
      description: "The generated answer text"
      minLength: 1
      example: "Machine learning has several important applications in healthcare, including medical image analysis for disease detection, predictive analytics for patient outcomes, drug discovery and development, and personalized treatment recommendations."
      
    context:
      type: "object"
      description: "Context used for answer generation"
      required:
        - "context_id"
        - "context_type"
      properties:
        context_id:
          type: "string"
          description: "Reference to the context artifact"
          example: "context:query123:001"
          
        context_type:
          type: "string"
          description: "Type of context used"
          enum: ["vector", "graph", "hybrid"]
          example: "hybrid"
          
        context_summary:
          type: "string"
          description: "Brief summary of the context"
          example: "Retrieved 5 chunks and 10 triplets about ML in healthcare"
          
        source_documents:
          type: "array"
          description: "List of source documents used"
          items:
            type: "object"
            properties:
              document_id:
                type: "string"
              title:
                type: "string"
              url:
                type: "string"
                format: "uri"
              relevance_score:
                type: "number"
                minimum: 0
                maximum: 1
          example:
            - document_id: "doc123"
              title: "Machine Learning in Healthcare"
              url: "https://example.com/ml-healthcare"
              relevance_score: 0.92
              
    generation_metadata:
      type: "object"
      description: "Metadata about the generation process"
      required:
        - "model"
        - "timestamp"
        - "generation_time_ms"
      properties:
        model:
          type: "string"
          description: "LLM model used for generation"
          example: "gpt-4"
          
        model_version:
          type: "string"
          description: "Version of the model"
          example: "gpt-4-0613"
          
        timestamp:
          type: "string"
          format: "date-time"
          description: "When the answer was generated"
          example: "2025-01-11T10:30:15Z"
          
        generation_time_ms:
          type: "number"
          description: "Time taken to generate the answer in milliseconds"
          minimum: 0
          example: 1250.5
          
        temperature:
          type: "number"
          description: "Temperature parameter used"
          minimum: 0
          maximum: 2
          example: 0.7
          
        max_tokens:
          type: "integer"
          description: "Maximum tokens for generation"
          minimum: 1
          example: 500
          
        tokens_used:
          type: "object"
          description: "Token usage breakdown"
          properties:
            prompt_tokens:
              type: "integer"
              example: 1200
            completion_tokens:
              type: "integer"
              example: 150
            total_tokens:
              type: "integer"
              example: 1350
              
        prompt_template:
          type: "string"
          description: "Template used for prompt construction"
          example: "Answer the following question based on the context: {context}\n\nQuestion: {query}\n\nAnswer:"
          
    citations:
      type: "array"
      description: "Citations and references in the answer"
      items:
        type: "object"
        required:
          - "citation_id"
          - "source_type"
          - "source_id"
        properties:
          citation_id:
            type: "string"
            description: "Unique citation identifier"
            example: "cite:001"
            
          source_type:
            type: "string"
            description: "Type of source"
            enum: ["chunk", "triplet", "document"]
            example: "chunk"
            
          source_id:
            type: "string"
            description: "ID of the source artifact"
            example: "chunk:doc123:001"
            
          text_span:
            type: "object"
            description: "Span in answer text that uses this citation"
            properties:
              start:
                type: "integer"
                minimum: 0
              end:
                type: "integer"
                minimum: 0
            example:
              start: 45
              end: 120
              
          confidence:
            type: "number"
            description: "Confidence in the citation"
            minimum: 0
            maximum: 1
            example: 0.95
            
    evaluation_metrics:
      type: "object"
      description: "Quality evaluation metrics for the answer"
      properties:
        faithfulness:
          type: "number"
          description: "Faithfulness to source context (0-1)"
          minimum: 0
          maximum: 1
          example: 0.92
          
        answer_relevance:
          type: "number"
          description: "Relevance to the query (0-1)"
          minimum: 0
          maximum: 1
          example: 0.88
          
        context_precision:
          type: "number"
          description: "Precision of context used (0-1)"
          minimum: 0
          maximum: 1
          example: 0.85
          
        context_recall:
          type: "number"
          description: "Recall of relevant context (0-1)"
          minimum: 0
          maximum: 1
          example: 0.90
          
        overall_score:
          type: "number"
          description: "Overall quality score (0-1)"
          minimum: 0
          maximum: 1
          example: 0.89
          
        evaluation_timestamp:
          type: "string"
          format: "date-time"
          description: "When evaluation was performed"
          example: "2025-01-11T10:30:20Z"
          
        evaluator:
          type: "string"
          description: "Evaluation method or model"
          example: "ragas-v0.1"
          
    feedback:
      type: "object"
      description: "User feedback on the answer"
      properties:
        rating:
          type: "integer"
          description: "User rating (1-5)"
          minimum: 1
          maximum: 5
          example: 4
          
        helpful:
          type: "boolean"
          description: "Whether user found answer helpful"
          example: true
          
        feedback_text:
          type: "string"
          description: "Free-form feedback"
          example: "Good answer but could include more specific examples"
          
        feedback_timestamp:
          type: "string"
          format: "date-time"
          description: "When feedback was provided"
          example: "2025-01-11T10:35:00Z"
          
    provenance:
      type: "object"
      description: "Full provenance chain"
      properties:
        retrieval_method:
          type: "string"
          enum: ["vector", "graph", "hybrid"]
          example: "hybrid"
          
        retrieval_config:
          type: "object"
          additionalProperties: true
          example:
            top_k: 5
            similarity_threshold: 0.7
            
        generation_config:
          type: "object"
          additionalProperties: true
          example:
            temperature: 0.7
            max_tokens: 500
            
        pipeline_version:
          type: "string"
          description: "Version of the RAG pipeline"
          example: "v1.2.0"
          
        experiment_id:
          type: "string"
          description: "Experiment or A/B test identifier"
          example: "exp:hybrid_rag_v2"
          
    status:
      type: "object"
      description: "Answer status and lifecycle"
      properties:
        state:
          type: "string"
          description: "Current state of the answer"
          enum: ["draft", "generated", "validated", "published", "archived"]
          example: "published"
          
        validated:
          type: "boolean"
          description: "Whether answer has been validated"
          default: false
          example: true
          
        validated_by:
          type: "string"
          description: "Who validated the answer"
          example: "human_reviewer"
          
        validated_at:
          type: "string"
          format: "date-time"
          description: "When validation occurred"
          example: "2025-01-11T11:00:00Z"
          
        validation_notes:
          type: "string"
          description: "Notes from validation"
          example: "Accurate and well-cited"

validation_rules:
  - rule: "answer_id_format"
    description: "Answer ID must follow naming convention"
    expression: "answer_id matches pattern 'answer:query:index'"
    
  - rule: "answer_not_empty"
    description: "Answer text must not be empty"
    expression: "answer_text.length > 0"
    
  - rule: "valid_metrics"
    description: "All evaluation metrics must be between 0 and 1"
    expression: "all metrics in evaluation_metrics are between 0 and 1"
    
  - rule: "citation_spans_valid"
    description: "Citation spans must be within answer text length"
    expression: "all citations have valid text spans"

examples:
  - name: "basic_generated_answer"
    description: "Basic generated answer with minimal fields"
    data:
      answer_id: "answer:query123:001"
      query:
        query_id: "query:123"
        query_text: "What are the applications of machine learning in healthcare?"
        query_timestamp: "2025-01-11T10:30:00Z"
      answer_text: "Machine learning has several important applications in healthcare, including medical image analysis for disease detection, predictive analytics for patient outcomes, drug discovery and development, and personalized treatment recommendations."
      context:
        context_id: "context:query123:001"
        context_type: "hybrid"
        context_summary: "Retrieved 5 chunks and 10 triplets about ML in healthcare"
      generation_metadata:
        model: "gpt-4"
        timestamp: "2025-01-11T10:30:15Z"
        generation_time_ms: 1250.5
        temperature: 0.7
        max_tokens: 500
        tokens_used:
          prompt_tokens: 1200
          completion_tokens: 150
          total_tokens: 1350
          
  - name: "enriched_generated_answer"
    description: "Fully enriched answer with citations and evaluation"
    data:
      answer_id: "answer:query456:002"
      query:
        query_id: "query:456"
        query_text: "How does deep learning improve medical diagnosis?"
        query_timestamp: "2025-01-11T11:00:00Z"
        user_id: "user:jane_doe"
        session_id: "session:xyz789"
      answer_text: "Deep learning significantly improves medical diagnosis through advanced image analysis capabilities. Convolutional neural networks can detect patterns in medical images that may be invisible to human observers, leading to earlier and more accurate disease detection."
      context:
        context_id: "context:query456:002"
        context_type: "hybrid"
        context_summary: "Retrieved 8 chunks and 15 triplets about deep learning in diagnosis"
        source_documents:
          - document_id: "doc789"
            title: "Deep Learning in Medical Imaging"
            url: "https://example.com/dl-medical"
            relevance_score: 0.95
      generation_metadata:
        model: "gpt-4"
        model_version: "gpt-4-0613"
        timestamp: "2025-01-11T11:00:15Z"
        generation_time_ms: 1450.2
        temperature: 0.7
        max_tokens: 500
        tokens_used:
          prompt_tokens: 1500
          completion_tokens: 180
          total_tokens: 1680
      citations:
        - citation_id: "cite:001"
          source_type: "chunk"
          source_id: "chunk:doc789:003"
          text_span:
            start: 0
            end: 85
          confidence: 0.95
      evaluation_metrics:
        faithfulness: 0.94
        answer_relevance: 0.91
        context_precision: 0.88
        context_recall: 0.92
        overall_score: 0.91
        evaluation_timestamp: "2025-01-11T11:00:20Z"
        evaluator: "ragas-v0.1"
      feedback:
        rating: 5
        helpful: true
        feedback_text: "Excellent answer with clear explanations"
        feedback_timestamp: "2025-01-11T11:05:00Z"
      provenance:
        retrieval_method: "hybrid"
        retrieval_config:
          top_k: 8
          similarity_threshold: 0.75
        generation_config:
          temperature: 0.7
          max_tokens: 500
        pipeline_version: "v1.2.0"
        experiment_id: "exp:hybrid_rag_v2"
      status:
        state: "published"
        validated: true
        validated_by: "human_reviewer"
        validated_at: "2025-01-11T11:10:00Z"
        validation_notes: "Accurate, well-cited, and comprehensive"

usage_guidelines:
  - "Always include citations for factual claims"
  - "Track evaluation metrics for quality monitoring"
  - "Collect user feedback for continuous improvement"
  - "Maintain full provenance for reproducibility"
  - "Validate high-stakes answers through human review"
  - "Monitor token usage for cost optimization"

performance_considerations:
  generation:
    - "Use streaming for long answers"
    - "Implement caching for common queries"
    - "Monitor and optimize token usage"
  evaluation:
    - "Run evaluation asynchronously"
    - "Batch evaluation requests"
    - "Cache evaluation results"
  storage:
    - "Archive old answers periodically"
    - "Index by query patterns"
    - "Compress large answer texts"